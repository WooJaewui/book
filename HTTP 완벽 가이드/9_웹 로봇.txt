
웹 로봇.

247 ~ 286page.



# 웹 로봇.

웹 로봇.
사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램.



# 크롤러와 크롤링.

웹 크롤러.
한 웹 페이지에서 다음페이지로 이동하며 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.
웹 링크를 재귀적으로 따라가는 로봇을 크롤러 혹은 스파이더라고 부른다.

루트 집합 root set.
크롤러가 방문을 시작하는 URL들의 초기 집합을 루트 집합( root set ) 이라고 부른다.
모든 문서로 이어지게 되는 하나의 문서는 없기 때문에 여러개의 루트 집합을 통해 크롤링한다.

순환 피하기.
로봇이 웹을 크롤링할 때 루프나 순환에 빠지지 않도록 조심해야 한다.
로봇들이 어디에 방문했는지 알게 만들어서 순환을 피해야된다.
URL에 별칭을 만들어서 피할 수 있다.
모든 순환을 피하는 완벽한 방법은 없다.

URL 정규화하기.
포트 번호가 명시되지 않았다면 ':80' 을 추가한다.
모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
# 태그들을 제거한다.



 # 로봇의 HTTP.

로봇 HTTP.
다른 HTTP 클라이언트 프로그램과 크게 다르지 않다.
필요한 최소한의 HTTP만 보낸다.

요청 헤더 식별하기.
로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더를 사이트에게 보내준다.
User-Agent : 서버에게 요청을 만든 로봇의 이름을 말해준다.
From : 로봇의 사용자 / 관리자의 이메일 주소.
Accept : 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
Referer : 요청 URL을 포함한 문서의 URL을 제공한다.

응답 다루기.
로봇들은 최소한 일반적인 상태 코드를 다룰 수 있어야 한다.
로봇들은 엔티티 자체의 정보를 찾을 수 있다.



# 부적절하게 동작하는 로봇들.

폭주하는 로봇.
웹 서핑하는 사람보다 훨씬 빠르게 동작하는 로봇은 과도한 트래픽을 유도할 수 있다.

잘못된 URL.
존재하지 않는 URL에 대한 요청을 보내는 로봇이 있을 수 있다.

호기심이 지나친 로봇.
사적인 데이터에 대한 URL을 얻어 애플리케이션을 통해 접근하도록 만들 수 있다.



# 로봇 차단하기.

robots.txt 파일들.
웹 사이트에 robots.txt 파일이 존재한다면 로봇은 그 파일을 가져와서 처리해야 된다.

robots.txt 가져오기.
HTTP GET 메소드를 이용해 robots.txt 리소스를 가져온다.
robots.txt가 존재한다면 서버는 그 파일을 text / plain 본문으로 반환한다.



# 검색엔진.

로봇의 방향성.
수많은 데이터에 대한 검색엔진에 크롤링을 사용할 경우 어떠한 방식이 가장 효율적인지 생각해라.

풀 텍스트 색인.
단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 알려줄 수 있는 데이터베이스.

검색 결과를 정렬하고 보여주기.
주어진 단어와 관련이 많은 순서대로 관련도 랭킹을 매겨 정렬한다.

스푸핑.
검색 결과에서 더 높은 순위를 차지하기 위해 여러 가지 속임수들이 생성되었다.






